(1) Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?
Усреднение micro используется, когда в датасете есть баланс классов. Например, для классификации изображений рукописных цифр.
Усреднение macro используется, когда нужно дать всем классам одинаковый вес, независимо от того, сколько их находится в датасете. Это нужно в случаях, когда важны редкие события, например, выявление мошеннических транзакций.
Усреднение weighted используется, когда нужно дать классам вес согласно их количеству (зависимость от присутствия класса в выборке). Например, задача классификации животных на улице.

(2) В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?
XGBoost, LightGBM и CatBoost являются вариантами алгоритмов градиентного бустинга.
Сплиты: lightGBM предлагает градиентную одностороннюю выборку (GOSS); Catboost предлагает технику Minimal Variance Sampling (MVS), которая представляет собой взвешенную выборку версии Stochastic Gradient Boosting; XGboost не использует никаких методов взвешенной выборки, что делает его процесс разбиения более медленным по сравнению с GOSS и MVS.
Рост листьев: Catboost строит сбалансированное дерево (выбирается пара признак-сплит, которая приносит наименьшие потери); LightGBM использует рост дерева по листьям (выбирает для роста тот лист, который минимизирует потери, что позволяет вырастить несбалансированное дерево); XGboost разделяет до заданного гиперпараметра, а затем обрезает дерево в обратном направлении и удаляет части, за пределами которых нет положительного выигрыша.
Обработка пропущенных значений: Catboost имеет два режима обработки отсутствующих значений; в LightGBM и XGBoost недостающие значения будут распределяться в ту сторону, которая уменьшает потери в каждом разбиении
Метод важности признаков: Catboost имеет два метода (PredictionValuesChange – для неранжированных метрик, LossFunctionChange – для любой модели, но особенно полезен для моделей ранжирования); LightGBM и XGBoost имеют два похожих метода (Gain - улучшение точности для ветвей, где находится признак, Split/Frequency/Weight - вычисляет относительное количество раз, когда признак встречается во всех расщеплениях деревьев модели).
Обработка категориальных признаков: Catboost использует комбинацию одноточечного кодирования и расширенного среднего кодирования; LightGBM разделяет категориальные признаки, разбивая их категории на 2 подмножества; XGBoost не имеет встроенного метода для категориальных признаков.
LightGBM является самым быстрым из всех остальных алгоритмов,CatBoost превосходит остальные по точности.
